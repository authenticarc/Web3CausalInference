# === HTEtester.py (light + optuna) ===
from dataclasses import dataclass
import numpy as np
from sklearn.base import clone
from sklearn.model_selection import StratifiedKFold, KFold
from catboost import CatBoostClassifier, CatBoostRegressor
from econml.dr import ForestDRLearner
import optuna


class HTETesterLight:
    """
    è½»é‡ + nAUUC è°ƒå‚ä¸å¸¦é€‰æ‹©ç‰ˆ HTE Tester

    ç›®æ ‡åœºæ™¯ï¼š
      - éœ€è¦æ¯”é»˜è®¤æ¨¡å‹æ›´å¼ºçš„ uplift æ’åºèƒ½åŠ›ï¼›
      - å¸Œæœ›ç”¨ nAUUC åšä¸€ä¸ªâ€œæ¨¡å‹å¥½åâ€çš„å®¢è§‚æ ‡å°ºï¼Œä½†ä¸è¿½æ±‚ç‰¹åˆ«é‡çš„ placebo / neg-controlã€‚

    åšçš„äº‹æƒ…ï¼š
      1) ï¼ˆå¯é€‰ï¼‰ç”¨ nAUUC + Optuna è°ƒ CatBoost reg/clf è¶…å‚ï¼ˆåœ¨ overlap å¸¦å†…ï¼‰
      2) cross-fitting ä¼°è®¡ Î¼1(x), Î¼0(x), e(x)
      3) è®¡ç®— AIPW ä¼ªæ•ˆåº” Ïˆ_i
      4) åœ¨ overlap å¸¦å†…è®­ç»ƒä¸€ä¸ª ForestDRLearnerï¼Œå¾—åˆ° Ï„Ì‚(x)
      5) æä¾›ç®€å•çš„ report() çœ‹ uplift åˆ†å¸ƒã€å¸¦è¦†ç›–æƒ…å†µã€Ï„Ì‚â€“Ïˆ ç›¸å…³

    ä¸»è¦å±æ€§ï¼ˆfit åï¼‰ï¼š
      - e_hat_      : OOF å€¾å‘å¾—åˆ† e(x)
      - mu1_hat_    : OOF é¢„æµ‹ Î¼1(x)
      - mu0_hat_    : OOF é¢„æµ‹ Î¼0(x)
      - psi_        : AIPW ä¼ªæ•ˆåº” Ïˆ_i
      - band_mask_  : overlap å¸¦æ©ç ï¼ˆåŸºäº e_hat_ å’Œ bandï¼‰
      - tau_hat_    : ForestDRLearner ä¼°è®¡ Ï„Ì‚(x)
      - dr_         : è®­ç»ƒå¥½çš„ ForestDRLearner å®ä¾‹
      - best_nauuc_ : è°ƒå‚å¾—åˆ°çš„æœ€ä½³ nAUUCï¼ˆå¦‚å¯ç”¨ï¼‰
      - best_params_: Optuna æœ€ä½³è¶…å‚ï¼ˆå¦‚å¯ç”¨ï¼‰
    """

    def __init__(
        self,
        regressor=None,
        classifier=None,
        n_splits: int = 5,
        trim: float = 0.01,
        band=(0.3, 0.7),         # e(x) overlap åˆå§‹å¸¦
        random_state: int = 42,
        verbose: int = 1,
        # ==== æ–°å¢ï¼šnAUUC è°ƒå‚ç›¸å…³ ====
        tune_nauuc: bool = False,
        n_trials: int = 40,
        early_stop: float = 0.45,   # nAUUC è¾¾åˆ°è¿™ä¸ªå°±åœ
    ):
        self.n_splits = int(n_splits)
        self.trim = float(trim)
        self.band = tuple(band)
        self.random_state = int(random_state)
        self.verbose = int(verbose)
        self.tune_nauuc = bool(tune_nauuc)
        self.n_trials = int(n_trials)
        self.early_stop = early_stop

        # åŸºç¡€ learnerï¼ˆå¯è¢«è°ƒå‚è¦†ç›–ï¼‰
        self.regressor = regressor or CatBoostRegressor(
            depth=8,
            learning_rate=0.05,
            l2_leaf_reg=3.0,
            subsample=0.9,
            verbose=0,
            random_seed=self.random_state,
            loss_function="RMSE",
        )
        self.classifier = classifier or CatBoostClassifier(
            depth=6,
            learning_rate=0.05,
            l2_leaf_reg=3.0,
            subsample=0.9,
            auto_class_weights="Balanced",
            verbose=0,
            random_seed=self.random_state,
        )

        # ForestDR é…ç½®ï¼šç»™ä¸€ä¸ªä¸­ç­‰è§„æ¨¡ï¼Œæ—¢ä¸å¤ªæ…¢ä¹Ÿæœ‰è¡¨è¾¾åŠ›
        self.forest_params = dict(
            n_estimators=600,
            max_depth=10,
            min_samples_split=4,
            min_samples_leaf=10,
            max_features="sqrt",
            max_samples=0.4,
            min_balancedness_tol=0.5,
            honest=True,
            subforest_size=4,
            cv=3,
            min_propensity=1e-3,
            categories="auto",
            random_state=self.random_state,
            n_jobs=-1,
        )

        # è¾“å‡º / ç¼“å­˜
        self._fitted = False
        self.dr_ = None
        self.e_hat_ = None
        self.mu1_hat_ = None
        self.mu0_hat_ = None
        self.psi_ = None
        self.band_mask_ = None
        self.tau_hat_ = None

        # è°ƒå‚ç»“æœ
        self.best_nauuc_ = None
        self.best_params_ = None

    # ---------- å·¥å…· ----------
    @staticmethod
    def _aipw_pseudo(y, t, mu1, mu0, e, trim=1e-3):
        """AIPW ä¼ªæ•ˆåº” Ïˆ_i"""
        e = np.clip(e, trim, 1 - trim)
        return (t * (y - mu1) / e) - ((1 - t) * (y - mu0) / (1 - e)) + (mu1 - mu0)

    def _crossfit_mu_e(self, X, T, Y):
        """
        ç”¨åŒä¸€å¥— StratifiedKFold åš OOF:
          - e_hat_: å€¾å‘å¾—åˆ†
          - mu1_hat_: Î¼1(x)
          - mu0_hat_: Î¼0(x)
        """
        n = len(Y)
        e_hat = np.zeros(n, dtype=float)
        mu1_hat = np.zeros(n, dtype=float)
        mu0_hat = np.zeros(n, dtype=float)

        skf = StratifiedKFold(
            n_splits=self.n_splits,
            shuffle=True,
            random_state=self.random_state,
        )

        for fold_id, (tr, va) in enumerate(skf.split(X, T), 1):
            if self.verbose >= 2:
                print(f"[CF] fold {fold_id}/{self.n_splits}")

            X_tr, X_va = X[tr], X[va]
            T_tr, Y_tr = T[tr], Y[tr]

            # å€¾å‘æ¨¡å‹
            clf = clone(self.classifier)
            if hasattr(clf, "set_params"):
                try:
                    clf.set_params(thread_count=1)
                except Exception:
                    pass
            clf.fit(X_tr, T_tr)
            e_hat[va] = clf.predict_proba(X_va)[:, 1]

            # ç»“æœæ¨¡å‹ï¼šåˆ†åˆ«åœ¨ treated / control ä¸Šå›å½’
            reg1 = clone(self.regressor)
            reg0 = clone(self.regressor)
            for mdl in (reg1, reg0):
                if hasattr(mdl, "set_params"):
                    try:
                        mdl.set_params(thread_count=1)
                    except Exception:
                        pass

            if (T_tr == 1).any():
                reg1.fit(X_tr[T_tr == 1], Y_tr[T_tr == 1])
                mu1_hat[va] = reg1.predict(X_va)
            else:
                mu1_hat[va] = Y_tr.mean()

            if (T_tr == 0).any():
                reg0.fit(X_tr[T_tr == 0], Y_tr[T_tr == 0])
                mu0_hat[va] = reg0.predict(X_va)
            else:
                mu0_hat[va] = Y_tr.mean()

        # è£å‰ª e_hat æå‡ç¨³å®šæ€§
        e_hat = np.clip(e_hat, self.trim, 1 - self.trim)
        return mu1_hat, mu0_hat, e_hat

    # === è°ƒå‚ä¸“ç”¨ï¼šæ›´è½»é‡çš„ OOF e(x) ===
    def _oof_propensity(self, X, T, clf_proto):
        e_oof = np.zeros(len(T), float)
        skf = StratifiedKFold(
            n_splits=self.n_splits,
            shuffle=True,
            random_state=self.random_state,
        )
        for tr, va in skf.split(X, T):
            clf = clone(clf_proto)
            if hasattr(clf, "set_params"):
                try:
                    clf.set_params(thread_count=1)
                except Exception:
                    pass
            clf.fit(X[tr], T[tr])
            e_oof[va] = clf.predict_proba(X[va])[:, 1]
        return np.clip(e_oof, self.trim, 1 - self.trim)

    # === è°ƒå‚ä¸“ç”¨ï¼šåœ¨ band å†…ç®— OOF nAUUC ===
    def _oof_nauuc_on_band(self, Xb, Tb, Yb, reg_proto, clf_proto):
        psi_oof = np.full(len(Xb), np.nan)
        tau_oof = np.full(len(Xb), np.nan)

        kf = KFold(
            n_splits=self.n_splits,
            shuffle=True,
            random_state=self.random_state,
        )

        for tr, va in kf.split(Xb):
            X_tr, X_va = Xb[tr], Xb[va]
            T_tr, T_va = Tb[tr], Tb[va]
            Y_tr, Y_va = Yb[tr], Yb[va]

            # å€¾å‘
            clf = clone(clf_proto)
            if hasattr(clf, "set_params"):
                try:
                    clf.set_params(thread_count=1)
                except Exception:
                    pass
            clf.fit(X_tr, T_tr)
            e_va = np.clip(clf.predict_proba(X_va)[:, 1], self.trim, 1 - self.trim)

            # ç»“æœå›å½’ Î¼1, Î¼0
            reg1 = clone(reg_proto)
            reg0 = clone(reg_proto)
            for mdl in (reg1, reg0):
                if hasattr(mdl, "set_params"):
                    try:
                        mdl.set_params(thread_count=1)
                    except Exception:
                        pass

            mu1_va = np.zeros(len(va))
            mu0_va = np.zeros(len(va))
            if (T_tr == 1).any():
                reg1.fit(X_tr[T_tr == 1], Y_tr[T_tr == 1])
                mu1_va = reg1.predict(X_va)
            if (T_tr == 0).any():
                reg0.fit(X_tr[T_tr == 0], Y_tr[T_tr == 0])
                mu0_va = reg0.predict(X_va)

            psi_oof[va] = self._aipw_pseudo(Y_va, T_va, mu1_va, mu0_va, e_va, trim=self.trim)

            # DR uplift Ï„Ì‚_oof
            if (T_tr == 1).any() and (T_tr == 0).any():
                dr = ForestDRLearner(
                    model_regression=clone(reg_proto),
                    model_propensity=clone(clf_proto),
                    **self.forest_params,
                )
                dr.fit(Y_tr, T_tr, X=X_tr)
                tau_oof[va] = dr.effect(X_va)

        m = ~np.isnan(psi_oof) & ~np.isnan(tau_oof)
        if m.sum() < max(30, 2 * self.n_splits):
            return 0.0, dict(note="too few valid oof points", n=int(m.sum()))

        psi_m = psi_oof[m]
        tau_m = tau_oof[m]

        # è®¡ç®— centered cumulative gain çš„é¢ç§¯
        def _area_cumgain_centered(psi, scores):
            order = np.argsort(-scores)
            psi_ord = psi[order]
            psi_c = psi_ord - psi_ord.mean()
            csum = np.cumsum(psi_c)
            x = np.arange(1, len(csum) + 1) / len(csum)
            return float(np.trapz(csum, x))

        area_model = _area_cumgain_centered(psi_m, tau_m)
        area_oracle = _area_cumgain_centered(psi_m, psi_m)
        if abs(area_oracle) < 1e-12:
            nauuc = 0.0
        else:
            nauuc = float(np.clip(area_model / area_oracle, 0.0, 1.0))

        return nauuc, dict(area_model=area_model, area_oracle=area_oracle, n=int(m.sum()))

    # === ç”¨ nAUUC è°ƒå‚ CatBoost è¶…å‚ ===
    def _tune_by_nauuc(self, X, T, Y):
        if self.verbose:
            print("[HTETesterLight] nAUUC tuning with Optuna...")

        # åŸºäºå½“å‰ classifier çš„ä¿åº• band
        e_base = self._oof_propensity(X, T, self.classifier)
        lo0, hi0 = self.band
        base_band = (e_base >= lo0) & (e_base <= hi0)
        if base_band.sum() < max(100, 3 * self.n_splits):
            base_band = np.ones_like(e_base, dtype=bool)

        def objective(trial):
            # ç®€åŒ–ç‰ˆæœç´¢ç©ºé—´ï¼ˆé¿å…å¤ªé‡ï¼‰
            reg = CatBoostRegressor(
                depth=trial.suggest_int("reg_depth", 4, 8),
                learning_rate=trial.suggest_float("reg_lr", 1e-3, 0.2, log=True),
                l2_leaf_reg=trial.suggest_float("reg_l2", 1e-2, 10.0, log=True),
                subsample=trial.suggest_float("reg_subsample", 0.7, 1.0),
                loss_function="RMSE",
                random_seed=self.random_state,
                verbose=0,
            )
            clf = CatBoostClassifier(
                depth=trial.suggest_int("clf_depth", 3, 8),
                learning_rate=trial.suggest_float("clf_lr", 1e-3, 0.2, log=True),
                l2_leaf_reg=trial.suggest_float("clf_l2", 1e-2, 10.0, log=True),
                subsample=trial.suggest_float("clf_subsample", 0.7, 1.0),
                auto_class_weights=trial.suggest_categorical(
                    "clf_class_wt", [None, "Balanced", "SqrtBalanced"]
                ),
                random_seed=self.random_state,
                verbose=0,
            )

            # ç”¨è¿™ä¸ª clf ç®— OOF e(x)ï¼Œé‡æ–°å®šä¹‰ band
            e_oof = self._oof_propensity(X, T, clf)
            lo, hi = self.band
            band = (e_oof >= lo) & (e_oof <= hi)
            if band.sum() < max(100, 3 * self.n_splits):
                band = base_band

            Xb, Tb, Yb = X[band], T[band], Y[band]
            nauuc, stats = self._oof_nauuc_on_band(Xb, Tb, Yb, reg, clf)

            # è½»å¾®æƒ©ç½šè¿‡çª„çš„ band
            cov = float(band.mean())
            if cov < 0.25:
                nauuc *= 0.9

            trial.set_user_attr("coverage", cov)
            trial.set_user_attr("n_eff", stats.get("n", 0))

            # æå‰åœæ­¢
            if (self.early_stop is not None) and (nauuc >= self.early_stop):
                trial.study.stop()
            return float(nauuc)

        def stop_callback(study, trial):
            if study.best_value is not None and study.best_value >= 0.35:
                print(
                    f"ğŸ‰ æå‰åœæ­¢ï¼šbest nAUUC={study.best_value:.3f} â‰¥ 0.35"
                )
                study.stop()

        pruner = optuna.pruners.MedianPruner(n_startup_trials=10)
        study = optuna.create_study(direction="maximize", pruner=pruner)
        study.optimize(
            objective,
            n_trials=self.n_trials,
            show_progress_bar=(self.verbose > 0),
            n_jobs=1,
            callbacks=[stop_callback],
        )

        best = study.best_trial
        self.best_nauuc_ = float(study.best_value)
        self.best_params_ = best.params

        if self.verbose:
            cov = best.user_attrs.get("coverage", float("nan"))
            neff = best.user_attrs.get("n_eff", 0)
            print(
                f"[HTETesterLight] tuning done. best nAUUC={self.best_nauuc_:.3f}, "
                f"coverage={cov:.2%}, n_eff={neff}"
            )
            print(f"[HTETesterLight] best params: {self.best_params_}")

        # ç”¨æœ€ä½³è¶…å‚é‡å»º regressor / classifier
        self.regressor = CatBoostRegressor(
            depth=best.params["reg_depth"],
            learning_rate=best.params["reg_lr"],
            l2_leaf_reg=best.params["reg_l2"],
            subsample=best.params["reg_subsample"],
            loss_function="RMSE",
            random_seed=self.random_state,
            verbose=0,
        )
        self.classifier = CatBoostClassifier(
            depth=best.params["clf_depth"],
            learning_rate=best.params["clf_lr"],
            l2_leaf_reg=best.params["clf_l2"],
            subsample=best.params["clf_subsample"],
            auto_class_weights=best.params["clf_class_wt"],
            random_seed=self.random_state,
            verbose=0,
        )

    # ---------- è®­ç»ƒ ----------
    def fit(self, X, T, Y):
        """
        è®­ç»ƒ ForestDRLearner å¹¶è¿”å›è®­ç»ƒå¥½çš„ dr æ¨¡å‹ã€‚
        åŒæ—¶ç¼“å­˜ï¼š
          - e_hat_, mu1_hat_, mu0_hat_, psi_, band_mask_, tau_hat_
        """
        X = np.asarray(X)
        T = np.asarray(T).astype(int)
        Y = np.asarray(Y).astype(float)

        if self.tune_nauuc:
            # å…ˆè°ƒå‚å†æ­£å¼ cross-fit
            self._tune_by_nauuc(X, T, Y)

        if self.verbose >= 1:
            print(f"[HTETester] n={len(Y)}, n_splits={self.n_splits}, band={self.band}")

        # 1) cross-fit Î¼1, Î¼0, e
        mu1_hat, mu0_hat, e_hat = self._crossfit_mu_e(X, T, Y)

        # 2) AIPW ä¼ªæ•ˆåº” Ïˆ_i
        psi = self._aipw_pseudo(Y, T, mu1_hat, mu0_hat, e_hat, trim=self.trim)

        # 3) æ ¹æ® e_hat å®šä¹‰ overlap å¸¦
        lo, hi = self.band
        band_mask = (e_hat >= lo) & (e_hat <= hi)
        if band_mask.sum() < max(50, 3 * self.n_splits):
            if self.verbose:
                print(f"[HTETester] band too narrow, keeping all samples.")
            band_mask = np.ones_like(e_hat, dtype=bool)

        if self.verbose >= 1:
            print(
                f"[HTETester] overlap band eâˆˆ[{lo:.2f},{hi:.2f}] è¦†ç›–ç‡={band_mask.mean():.2%}, "
                f"n_band={band_mask.sum()}"
            )

        # 4) åœ¨å¸¦å†…è®­ç»ƒ DR-Learner
        dr = ForestDRLearner(
            model_regression=clone(self.regressor),
            model_propensity=clone(self.classifier),
            **self.forest_params,
        )
        dr.fit(Y[band_mask], T[band_mask], X=X[band_mask])

        # 5) å¯¹å…¨ä½“æ ·æœ¬æ‰“ Ï„Ì‚(x)
        tau_hat = dr.effect(X)

        # 6) ç¼“å­˜
        self.mu1_hat_ = mu1_hat
        self.mu0_hat_ = mu0_hat
        self.e_hat_ = e_hat
        self.psi_ = psi
        self.band_mask_ = band_mask
        self.tau_hat_ = tau_hat
        self.dr_ = dr
        self._fitted = True

        return dr

    # ---------- æŠ¥å‘Š ----------
    def report(self, quantiles=(0.1, 0.25, 0.5, 0.75, 0.9)) -> str:
        """
        fit() ä¹‹åè°ƒç”¨ï¼Œè¾“å‡ºä¸€ä¸ªç®€æ˜“ HTE æŠ¥å‘Šï¼š
          - overlap å¸¦è¦†ç›–æƒ…å†µ
          - Ï„Ì‚(x) åˆ†å¸ƒï¼ˆå…¨ä½“ & å¸¦å†…ï¼‰
          - Ïˆ ä¸ Ï„Ì‚ çš„ç›¸å…³æ€§ï¼ˆå¸¦å†…ï¼‰
          - å¦‚è°ƒå‚ï¼Œé™„å¸¦ best nAUUC
        """
        assert self._fitted, "è¯·å…ˆè°ƒç”¨ fit(X, T, Y)ã€‚"

        e = self.e_hat_
        psi = self.psi_
        tau = self.tau_hat_
        band = self.band_mask_
        lo, hi = self.band

        lines = []
        lines.append("ã€HTE è¯Šæ–­æŠ¥å‘Šï¼ˆè½»é‡ + è°ƒå‚ç‰ˆï¼‰ã€‘")
        lines.append(
            f"- overlap å¸¦ eâˆˆ[{lo:.2f},{hi:.2f}] è¦†ç›–ç‡={band.mean():.2%}, "
            f"n_band={int(band.sum())}"
        )

        if self.best_nauuc_ is not None:
            lines.append(f"- è°ƒå‚ best nAUUC = {self.best_nauuc_:.3f}")

        # uplift åˆ†å¸ƒ
        def _fmt_q(arr, name):
            arr = np.asarray(arr, float)
            qs = np.quantile(arr, quantiles)
            q_str = " / ".join(
                [f"{int(q*100)}%={v:.3f}" for q, v in zip(quantiles, qs)]
            )
            return f"  Â· {name} quantiles: {q_str}"

        lines.append(f"- Ï„Ì‚(x) åˆ†å¸ƒï¼š")
        lines.append(_fmt_q(tau, "å…¨ä½“"))
        lines.append(_fmt_q(tau[band], "å¸¦å†…"))

        # Ïˆ åˆ†å¸ƒï¼ˆæ›´å¤šæ˜¯ sanity checkï¼‰
        lines.append(f"- Ïˆ (AIPW ä¼ªæ•ˆåº”) åˆ†å¸ƒï¼ˆå¸¦å†…ï¼‰ï¼š")
        lines.append(_fmt_q(psi[band], "Ïˆ å¸¦å†…"))

        # Ï„Ì‚ ä¸ Ïˆ çš„ç›¸å…³æ€§ï¼ˆå¸¦å†…ï¼‰
        try:
            corr = np.corrcoef(tau[band], psi[band])[0, 1]
            lines.append(f"- Ï„Ì‚ ä¸ Ïˆ åœ¨å¸¦å†…çš„ç›¸å…³ç³»æ•°ï¼šcorr(Ï„Ì‚, Ïˆ) = {corr:.3f}")
        except Exception:
            lines.append("- Ï„Ì‚ ä¸ Ïˆ ç›¸å…³æ€§ï¼šè®¡ç®—å¤±è´¥ï¼ˆæ ·æœ¬è¿‡å°‘æˆ–å…¨å¸¸æ•°ï¼‰")

        return "\n".join(lines)

    # ---------- å–å›è®­ç»ƒå¥½çš„ DR æ¨¡å‹ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰ ----------
    def get_model(self):
        assert self._fitted and (self.dr_ is not None), "è¯·å…ˆè°ƒç”¨ fit(X, T, Y)ã€‚"
        return self.dr_
