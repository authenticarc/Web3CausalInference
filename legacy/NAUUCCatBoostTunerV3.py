# -*- coding: utf-8 -*-
"""
NAUUCCatBoostTunerV3
- ç›®æ ‡ï¼šåœ¨ä¸æ”¹å˜å¯¹å¤– API çš„å‰æä¸‹ï¼ˆä»è¿”å› reg / clfï¼‰ï¼Œ
  åˆ©ç”¨ DR ä½œä¸ºå› æœä¸»ä½“ï¼ŒåŒæ—¶å¼•å…¥ Ranker(Ïˆ) å­¦æ’åºï¼Œåœ¨è°ƒå‚é˜¶æ®µ
  ç”¨ â€œscore = (1-eta)*tau_DR + eta*score_rankerâ€ è®¡ç®— nAUUC ä½œä¸ºè¯„åˆ†æŒ‡æ ‡ã€‚
- ä½ å¯ä»¥æŠŠæœ¬æ–‡ä»¶å‘½åä¸º nauuctunerv2.pyï¼Œæ›¿æ¢/å¹¶å­˜äºç°æœ‰å·¥ç¨‹ä¸­ã€‚
"""

from __future__ import annotations
import numpy as np
import optuna
from typing import Optional, Tuple
from sklearn.model_selection import KFold, StratifiedKFold
from catboost import CatBoostRegressor, CatBoostClassifier, CatBoostRanker, Pool
from econml.dr import ForestDRLearner


class NAUUCCatBoostTunerV3:
    """
    ç”¨ nAUUC è¿›è¡Œè°ƒå‚ï¼Œä½†è¯„åˆ†ç”¨ â€œDR çš„ tauÌ‚ ä¸ Ranker(Ïˆ) çš„èåˆåˆ†æ•°â€ï¼š
        score = (1 - eta) * tau_DR + eta * score_ranker,  etaâˆˆ[0,1]

    å¯¹å¤– APIï¼š
        tuner = NAUUCCatBoostTunerV3(n_trials=60, n_splits=5, random_state=42, eta=0.3, verbose=True)
        best_reg, best_clf = tuner.fit_return_models(X, T, Y)

    æ³¨æ„ï¼š
    - è¿”å›çš„ best_reg / best_clf ä»æ˜¯â€œæœªæ‹Ÿåˆâ€å®ä¾‹ï¼›
    - å®é™…çº¿ä¸Šä¼°è®¡ä¸ä¸‹æ¸¸ä½¿ç”¨ä¾æ—§ç”¨ DRï¼ˆForestDRLearnerï¼‰ï¼Œä¿è¯å› æœä¸€è‡´æ€§ï¼›
    - Ranker ä»…ç”¨äºâ€œè°ƒå‚è¯„åˆ†é˜¶æ®µâ€çš„æ’åºå­¦ä¹ ï¼Œä¸å‚ä¸å¯¹å¤–æ¨ç†ã€‚
    """

    def __init__(
        self,
        n_trials: int = 50,
        n_splits: int = 5,
        random_state: int = 42,
        categories: Optional[str] = "auto",
        trim: float = 1e-3,       # e(x) è£å‰ªç”¨äº Ïˆ/Ï† è®¡ç®—çš„ç¨³å®šåŒ–
        eta: float = 0.30,        # æ’åºèåˆæƒé‡ï¼š0=çº¯DRï¼Œ1=çº¯Ranker(Ïˆ)
        lambda_ci: float = 0.1,   # å¯é€‰ï¼šå¯¹ CI å®½åº¦çš„è½»æƒ©ç½šï¼›é»˜è®¤ 0 ä¸å¯ç”¨
        verbose: bool = False,
        reg_lf: str = None  # ä»…ç”¨äºè¿”å›çš„ best_reg çš„é»˜è®¤æŸå¤±å‡½æ•°
    ):
        self.n_trials = n_trials
        self.n_splits = n_splits
        self.random_seed = random_state
        self.categories = categories
        self.trim = float(trim)
        self.eta = float(np.clip(eta, 0.0, 1.0))
        self.lambda_ci = float(max(0.0, lambda_ci))
        self.verbose = verbose
        self.reg_lf = reg_lf

        # ç»“æœç¼“å­˜
        self.best_params_: Optional[dict] = None
        self.best_value_: Optional[float] = None
        self.best_reg_: Optional[CatBoostRegressor] = None
        self.best_clf_: Optional[CatBoostClassifier] = None

    # ---------- AIPW ä¼ªæ•ˆåº” Ïˆ / å½±å“å‡½æ•° Ï† / nAUUC ---------- #
    @staticmethod
    def _aipw_pseudo(y, t, mu1, mu0, e, trim=1e-3):
        e = np.clip(e, trim, 1 - trim)
        return (t * (y - mu1) / e) - ((1 - t) * (y - mu0) / (1 - e)) + (mu1 - mu0)

    @staticmethod
    def _phi(y, t, mu1, mu0, e, trim=1e-3):
        e = np.clip(e, trim, 1 - trim)
        return (mu1 - mu0) + t * (y - mu1) / e - (1 - t) * (y - mu0) / (1 - e)

    @staticmethod
    def _area_from_scores(psi, score):
        order = np.argsort(-score)
        psi_ord = psi[order]
        csum = np.cumsum(psi_ord)
        x = np.arange(1, len(psi_ord) + 1) / len(psi_ord)
        return float(np.trapz(csum, x))

    @staticmethod
    def _nauuc_from(psi, score):
        a_model = NAUUCCatBoostTunerV3._area_from_scores(psi, score)
        a_oracle = NAUUCCatBoostTunerV3._area_from_scores(psi, psi)
        if a_oracle <= 0:
            return 0.0
        return float(np.clip(a_model / a_oracle, 0.0, 1.0))

    # ---------- æ„å»ºå›å½’æŸå¤±ï¼ˆå« Tweedieï¼‰ ---------- #
    @staticmethod
    def _build_reg_loss(loss_name: str, tweedie_p: Optional[float]) -> Tuple[str, str]:
        if loss_name == "RMSE":
            return "RMSE", "RMSE"
        p = 1.3 if tweedie_p is None else float(tweedie_p)
        lf = f"Tweedie:variance_power={p}"
        return lf, lf

    # ---------- Optuna ç›®æ ‡ï¼šnAUUC(fused score) - Î»Â·CIwidth(norm) ---------- #
    def _objective(self, trial, X, T, Y):
        # å…±ç”¨ & æ—©åœå‚æ•°
        iterations = trial.suggest_int("iterations", 300, 2000, log=True)
        od_wait = trial.suggest_int("od_wait", 50, 200)
        depth_reg = trial.suggest_int("reg_depth", 4, 10)
        depth_clf = trial.suggest_int("clf_depth", 3, 8)

        # å›å½’æŸå¤±ï¼ˆå« Tweedieï¼‰
        if self.reg_lf == 'RMSE':
            reg_loss = trial.suggest_categorical("reg_loss", ["RMSE"])
        else:
            reg_loss = trial.suggest_categorical("reg_loss", ["RMSE", "Tweedie"])
        
        reg_tweedie_p = None
        if reg_loss == "Tweedie":
            reg_tweedie_p = trial.suggest_float("reg_tweedie_p", 1.1, 1.9)
        reg_lf, _ = self._build_reg_loss(reg_loss, reg_tweedie_p)

        # CatBoost å‚æ•°
        reg_params = dict(
            iterations=iterations,
            depth=depth_reg,
            learning_rate=trial.suggest_float("reg_lr", 1e-3, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("reg_l2", 1e-3, 10.0, log=True),
            subsample=trial.suggest_float("reg_subsample", 0.7, 1.0),
            random_seed=self.random_seed,
            verbose=False,
            loss_function=reg_lf,
            od_type="Iter",
            od_wait=od_wait,
        )
        clf_params = dict(
            iterations=iterations,
            depth=depth_clf,
            learning_rate=trial.suggest_float("clf_lr", 1e-3, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("clf_l2", 1e-3, 10.0, log=True),
            subsample=trial.suggest_float("clf_subsample", 0.7, 1.0),
            auto_class_weights=trial.suggest_categorical("clf_class_wt", ["Balanced", "SqrtBalanced", None]),
            random_seed=self.random_seed,
            verbose=False,
            loss_function="Logloss",
            od_type="Iter",
            od_wait=od_wait,
        )

        # ä½¿ç”¨åˆ†å±‚ K æŠ˜ï¼Œä¿è¯å„æŠ˜ T æ¯”ä¾‹æ¥è¿‘
        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_seed)

        # æ”¶é›†æŒ‡æ ‡
        nauuc_folds = []
        phi_all = []

        X = np.asarray(X)
        T = np.asarray(T, dtype=int)
        Y = np.asarray(Y, dtype=float)

        for tr_idx, va_idx in skf.split(X, T):
            X_tr, X_va = X[tr_idx], X[va_idx]
            T_tr, T_va = T[tr_idx], T[va_idx]
            Y_tr, Y_va = Y[tr_idx], Y[va_idx]

            # ========== 1) eÌ‚/Î¼Ì‚ï¼šè®­ç»ƒæŠ˜æ‹Ÿåˆï¼Œè®­ç»ƒæŠ˜/éªŒè¯æŠ˜å„è‡ªé¢„æµ‹ ==========
            clf = CatBoostClassifier(**clf_params)
            clf.fit(X_tr, T_tr)
            e_tr = clf.predict_proba(X_tr)[:, 1]
            e_va = clf.predict_proba(X_va)[:, 1]

            reg1 = CatBoostRegressor(**reg_params)
            reg0 = CatBoostRegressor(**reg_params)
            reg1.fit(X_tr[T_tr == 1], Y_tr[T_tr == 1])
            reg0.fit(X_tr[T_tr == 0], Y_tr[T_tr == 0])

            mu1_tr = reg1.predict(X_tr)
            mu0_tr = reg0.predict(X_tr)
            mu1_va = reg1.predict(X_va)
            mu0_va = reg0.predict(X_va)

            # ========== 2) Ïˆï¼ˆAIPW ä¼ªæ•ˆåº”ï¼‰ï¼šè®­ç»ƒæŠ˜ç”¨äº ranker ç›‘ç£ï¼ŒéªŒè¯æŠ˜ç”¨äº nAUUC ==========
            psi_tr = self._aipw_pseudo(Y_tr, T_tr, mu1_tr, mu0_tr, e_tr, trim=self.trim)
            psi_va = self._aipw_pseudo(Y_va, T_va, mu1_va, mu0_va, e_va, trim=self.trim)

            # ========== 3) DR-Learnerï¼šå› æœä¸»ä½“ï¼Œç»™å‡º tauÌ‚ï¼ˆç”¨äºèåˆæ‰“åˆ†ï¼‰ ==========
            dr = ForestDRLearner(
                model_regression=CatBoostRegressor(**reg_params),
                model_propensity=CatBoostClassifier(**clf_params),
                random_state=self.random_seed,
                categories=self.categories,
            )
            dr.fit(Y_tr, T_tr, X=X_tr)
            tau_va = dr.effect(X_va).ravel()

            # ========== 4) Ranker(Ïˆ)ï¼šå­¦ä¹ æ’åºï¼Œåªç”¨äºè¯„åˆ†ï¼Œä¸æ›¿æ¢ DR ==========
            group_tr = np.ones(len(X_tr), dtype=int)
            group_va = np.ones(len(X_va), dtype=int)
            tr_pool = Pool(X_tr, label=psi_tr, group_id=group_tr)
            va_pool = Pool(X_va, label=psi_va, group_id=group_va)

            ranker = CatBoostRanker(
                loss_function="YetiRank",
                iterations=min(300, iterations),
                depth=min(6, reg_params["depth"]),
                learning_rate=0.1,
                random_seed=self.random_seed,
                verbose=False,
                od_type="Iter",
                od_wait=min(50, od_wait),
                sampling_frequency="PerTree"  # æ¯æ£µæ ‘é‡æ–°é‡‡æ · pairï¼Œè¿›ä¸€æ­¥å‡è½»å‹åŠ›
            )
            ranker.fit(tr_pool, eval_set=va_pool)
            score_rank = np.asarray(ranker.predict(va_pool)).ravel()

            # ========== 5) èåˆæ’åºå¾—åˆ†ï¼šscore = (1-eta)*tauÌ‚ + eta*ranker ==========
            score_fused = (1.0 - self.eta) * tau_va + self.eta * score_rank

            # ========== 6) nAUUC ==========
            nauuc = self._nauuc_from(psi_va, score_fused)
            nauuc_folds.append(nauuc)

            # ï¼ˆå¯é€‰ï¼‰CI æƒ©ç½šï¼šæ”¶é›† Ï†
            if self.lambda_ci > 0.0:
                phi_va = self._phi(Y_va, T_va, mu1_va, mu0_va, e_va, trim=self.trim)
                phi_all.append(phi_va)

        nauuc_mean = float(np.mean(nauuc_folds))

        # â€”â€” å¯é€‰ï¼šå¯¹ CI å®½åº¦åšè½»æƒ©ç½šï¼ˆé»˜è®¤ lambda_ci=0 å…³é—­ï¼‰â€”â€”
        if self.lambda_ci > 0.0 and len(phi_all) > 0:
            phi_all = np.concatenate(phi_all, axis=0)
            se = phi_all.std(ddof=1) / np.sqrt(len(phi_all))
            ciw = 2.0 * 1.96 * se
            y_scale = max(np.std(Y, ddof=1), 1e-8)
            ciw_norm = ciw / y_scale
            obj = nauuc_mean - self.lambda_ci * ciw_norm
        else:
            obj = nauuc_mean

        if self.verbose:
            msg = f"[trial] nAUUC(fused)={nauuc_mean:.3f}"
            if self.lambda_ci > 0.0 and len(phi_all) > 0:
                msg += f" | obj={obj:.3f}"
            print(msg)
        return obj

    # ---------- å¯¹å¤–æ¥å£ï¼šè¿”å›â€œæœªæ‹Ÿåˆâ€çš„æœ€ä½³ reg / clf ---------- #
    def fit_return_models(self, X, T, Y) -> Tuple[CatBoostRegressor, CatBoostClassifier]:
        # æ—©åœï¼šå½“ç›®æ ‡è¶³å¤Ÿé«˜æ—¶å¯ä»¥åœï¼ˆç»éªŒé˜ˆå€¼ï¼Œå¯æŒ‰éœ€è°ƒæ•´/åˆ é™¤ï¼‰
        def stop_cb(study, trial):
            if study.best_value is not None and study.best_value >= 0.55:
                print(f"ğŸ‰ æå‰åœæ­¢ï¼šobjectiveâ‰ˆ{study.best_value:.3f}")
                study.stop()

        study = optuna.create_study(direction="maximize")
        study.optimize(lambda tr: self._objective(tr, X, T, Y),
                       n_trials=self.n_trials,
                       show_progress_bar=self.verbose,
                       n_jobs=1,
                       callbacks=[stop_cb],
                       gc_after_trial=True)

        self.best_params_ = study.best_params
        self.best_value_ = study.best_value

        # è¿˜åŸå›å½’æŸå¤±
        reg_loss = self.best_params_.get("reg_loss", "RMSE")
        reg_tweedie_p = self.best_params_.get("reg_tweedie_p", None)
        reg_lf, _ = self._build_reg_loss(reg_loss, reg_tweedie_p)

        # æ„é€ â€œæœªæ‹Ÿåˆâ€çš„æœ€ä½³æ¨¡å‹å®ä¾‹
        self.best_reg_ = CatBoostRegressor(
            iterations=self.best_params_["iterations"],
            depth=self.best_params_["reg_depth"],
            learning_rate=self.best_params_["reg_lr"],
            l2_leaf_reg=self.best_params_["reg_l2"],
            subsample=self.best_params_["reg_subsample"],
            random_seed=self.random_seed,
            verbose=False,
            loss_function=reg_lf,
        )
        self.best_clf_ = CatBoostClassifier(
            iterations=self.best_params_["iterations"],
            depth=self.best_params_["clf_depth"],
            learning_rate=self.best_params_["clf_lr"],
            l2_leaf_reg=self.best_params_["clf_l2"],
            subsample=self.best_params_["clf_subsample"],
            auto_class_weights=self.best_params_.get("clf_class_wt", None),
            random_seed=self.random_seed,
            verbose=False,
            loss_function="Logloss",
        )
        return self.best_reg_, self.best_clf_
