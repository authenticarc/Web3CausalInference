import numpy as np
import optuna
from sklearn.model_selection import KFold
from catboost import CatBoostRegressor, CatBoostClassifier
from econml.dr import ForestDRLearner
from typing import Optional, Tuple


class NAUUCCatBoostTuner:
    """
    ç”¨ nAUUC (normalized AUUC, 0~1) ä½œä¸º Optuna ç›®æ ‡ï¼Œæœç´¢ CatBoost çš„
    å›å½’å™¨/åˆ†ç±»å™¨è¶…å‚ã€‚è¿”å›ä¸¤ä¸ªâ€œæœ€ä½³å‚æ•°é…ç½®â€çš„ CatBoost æ¨¡å‹å®ä¾‹
    ï¼ˆæœªæ‹Ÿåˆï¼Œç›´æ¥å¯ä¼ å…¥ econml çš„ DR/ForestDRLearner ä½¿ç”¨ï¼‰ã€‚

    ç”¨æ³•ï¼š
        tuner = NAUUCCatBoostTuner(n_trials=50, n_splits=5, random_seed=42)
        best_reg, best_clf = tuner.fit_return_models(X, T, Y)
        # ç„¶åï¼š
        dr = ForestDRLearner(model_regression=best_reg,
                             model_propensity=best_clf,
                             random_state=42, categories='auto')
        dr.fit(Y, T, X=X)
        tau = dr.effect(X)
    """

    def __init__(
        self,
        n_trials: int = 50,
        n_splits: int = 5,
        random_state: int = 42,
        categories: Optional[str] = "auto",
        trim: float = 1e-3,            # e(x) è£å‰ªï¼Œæå‡ç¨³å®šæ€§
        verbose: bool = False,
        reg_lf = None
    ):
        self.n_trials = n_trials
        self.n_splits = n_splits
        self.random_seed = random_state
        self.categories = categories
        self.trim = trim
        self.verbose = verbose

        self.best_params_: Optional[dict] = None
        self.best_value_: Optional[float] = None
        self.best_reg_: Optional[CatBoostRegressor] = None
        self.best_clf_: Optional[CatBoostClassifier] = None
        self.reg_lf = reg_lf

    # ---------- AIPW ä¼ªæ•ˆåº” Ïˆï¼šä¸€è‡´ä¼°è®¡çœŸæ•ˆåº”ï¼Œä½œæ ‡å‡†åŒ–åŸºå‡† ---------- #
    @staticmethod
    def _aipw_pseudo_outcome(y, t, mu1, mu0, e, trim=1e-3):
        e = np.clip(e, trim, 1 - trim)
        return (t * (y - mu1) / e) - ((1 - t) * (y - mu0) / (1 - e)) + (mu1 - mu0)

    # ---------- ç»™å®šâ€œæ’åºåˆ†æ•°â€æ—¶ï¼Œå¯¹ Ïˆ çš„ç´¯è®¡ç§¯åˆ†é¢ç§¯ ---------- #
    @staticmethod
    def _area_from_scores(psi, order_score):
        order = np.argsort(-order_score)
        psi_ord = psi[order]
        csum = np.cumsum(psi_ord)
        x = np.arange(1, len(psi_ord) + 1) / len(psi_ord)  # è¦†ç›–ç‡ [0,1]
        return float(np.trapz(csum, x))
    
     # ---------- æ„å»ºå›å½’æŸå¤±å­—ç¬¦ä¸² ---------- #
    @staticmethod
    def _build_reg_loss(loss_name: str, tweedie_p: Optional[float]) -> Tuple[str, str]:
        """
        è¿”å› (loss_function, eval_metric) å­—ç¬¦ä¸²
        """
        if loss_name == "RMSE":
            return "RMSE", "RMSE"
        # Tweedie
        p = 1.3 if tweedie_p is None else float(tweedie_p)
        lf = f"Tweedie:variance_power={p}"
        return lf, lf

    # ---------- Optuna çš„ç›®æ ‡ï¼šäº¤å‰éªŒè¯ä¸Šçš„ AIPW-nAUUC ---------- #
    def _objective(self, trial, X, T, Y):
        # ---- å…±ç”¨ & æ—©åœå‚æ•° ---- #
        iterations = trial.suggest_int("iterations", 300, 2000, log=True)
        od_wait = trial.suggest_int("od_wait", 50, 200)
        depth_reg = trial.suggest_int("reg_depth", 4, 10)
        depth_clf = trial.suggest_int("clf_depth", 3, 8)

        # å›å½’æŸå¤±é€‰æ‹©ï¼ˆåŒ…å« Tweedie çš„ variance_powerï¼‰
        if self.reg_lf == 'RMSE':
            reg_loss = trial.suggest_categorical("reg_loss", ["RMSE"])
        else:
            reg_loss = trial.suggest_categorical("reg_loss", ["RMSE", "Tweedie"])
        reg_tweedie_p = None
        if reg_loss == "Tweedie":
            reg_tweedie_p = trial.suggest_float("reg_tweedie_p", 1.1, 1.9)

        reg_lf, reg_eval = self._build_reg_loss(reg_loss, reg_tweedie_p)

        reg_params = dict(
            iterations=iterations,
            depth=depth_reg,
            learning_rate=trial.suggest_float("reg_lr", 1e-3, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("reg_l2", 1e-3, 10.0, log=True),
            subsample=trial.suggest_float("reg_subsample", 0.7, 1.0),
            random_seed=self.random_seed,
            verbose=False,
            loss_function=reg_lf,
            od_type="Iter",
            od_wait=od_wait,
        )

        clf_params = dict(
            iterations=iterations,
            depth=depth_clf,
            learning_rate=trial.suggest_float("clf_lr", 1e-3, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("clf_l2", 1e-3, 10.0, log=True),
            subsample=trial.suggest_float("clf_subsample", 0.7, 1.0),
            auto_class_weights=trial.suggest_categorical("clf_class_wt", ["Balanced", "SqrtBalanced", None]),
            random_seed=self.random_seed,
            verbose=False,
            loss_function="Logloss",
            od_type="Iter",
            od_wait=od_wait,
        )

        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_seed)
        scores = []

        X_np = np.asarray(X)
        T_np = np.asarray(T, dtype=int)
        Y_np = np.asarray(Y, dtype=float)

        for tr_idx, va_idx in kf.split(X_np):
            X_tr, X_va = X_np[tr_idx], X_np[va_idx]
            T_tr, T_va = T_np[tr_idx], T_np[va_idx]
            Y_tr, Y_va = Y_np[tr_idx], Y_np[va_idx]

            # 1) å€¾å‘ä¸ç»“æœæ¨¡å‹ï¼ˆåœ¨è®­ç»ƒæŠ˜ä¸Šæ‹Ÿåˆï¼ŒéªŒè¯æŠ˜ä¸Šé¢„æµ‹ï¼‰
            clf = CatBoostClassifier(**clf_params)
            clf.fit(X_tr, T_tr)
            e_va = clf.predict_proba(X_va)[:, 1]

            reg1 = CatBoostRegressor(**reg_params)
            reg0 = CatBoostRegressor(**reg_params)
            # æ³¨æ„ï¼šåªç”¨å¯¹åº”ç»„çš„æ•°æ®æ‹Ÿåˆ m1/m0
            reg1.fit(X_tr[T_tr == 1], Y_tr[T_tr == 1])
            reg0.fit(X_tr[T_tr == 0], Y_tr[T_tr == 0])
            mu1_va = reg1.predict(X_va)
            mu0_va = reg0.predict(X_va)

            # 2) AIPW ä¼ªæ•ˆåº” Ïˆï¼ˆæ ‡å‡†åŒ–åŸºå‡†ï¼‰
            psi_va = self._aipw_pseudo_outcome(Y_va, T_va, mu1_va, mu0_va, e_va, trim=self.trim)

            # 3) ç”¨åŒä¸€è¶…å‚åœ¨è®­ç»ƒæŠ˜ä¸Šæ‹Ÿåˆ DR-Learnerï¼Œå¾—åˆ° Ï„Ì‚ ä½œä¸ºæ¨¡å‹æ’åºåˆ†æ•°
            dr = ForestDRLearner(
                model_regression=CatBoostRegressor(**reg_params),
                model_propensity=CatBoostClassifier(**clf_params),
                random_state=self.random_seed,
                categories=self.categories,
            )
            dr.fit(Y_tr, T_tr, X=X_tr)
            tau_va = dr.effect(X_va)

            # 4) é¢ç§¯ï¼šæ¨¡å‹æŒ‰ Ï„Ì‚ æ’åº vs Oracle æŒ‰ Ïˆ æ’åº
            area_model = self._area_from_scores(psi_va, tau_va)
            area_oracle = self._area_from_scores(psi_va, psi_va)

            # 5) æ ‡å‡†åŒ–ï¼šnAUUC âˆˆ [0,1]
            if area_oracle <= 0:
                nauuc = 0.0
            else:
                nauuc = max(0.0, min(1.0, area_model / area_oracle))

            scores.append(nauuc)

        return float(np.mean(scores))

    # ---------- å¯¹å¤–æ¥å£ï¼šè¿è¡Œè°ƒå‚å¹¶è¿”å›ä¸¤ä¸ªâ€œæœ€ä½³å‚æ•°â€çš„æ¨¡å‹å®ä¾‹ ---------- #
    def fit_return_models(self, X, T, Y) -> Tuple[CatBoostRegressor, CatBoostClassifier]:
        def stop_callback(study, trial):
            if study.best_value is not None and study.best_value >= 0.45:
                print(f"ğŸ‰ æå‰åœæ­¢ï¼šbest nAUUC={study.best_value:.3f} â‰¥ 0.45")
                study.stop()
        study = optuna.create_study(direction="maximize")
        study.optimize(lambda tr: self._objective(tr, X, T, Y),
                       n_trials=self.n_trials,
                       show_progress_bar=self.verbose,n_jobs=1,callbacks=[stop_callback],
                       gc_after_trial=True)

        self.best_params_ = study.best_params
        self.best_value_ = study.best_value
        
        # è¿˜åŸæœ€ä¼˜å›å½’æŸå¤±
        reg_loss = self.best_params_.get("reg_loss", "RMSE")
        reg_tweedie_p = self.best_params_.get("reg_tweedie_p", None)
        reg_lf, reg_eval = self._build_reg_loss(reg_loss, reg_tweedie_p)

         # æ„é€ â€œæœªæ‹Ÿåˆâ€çš„æœ€ä½³æ¨¡å‹å®ä¾‹ï¼ˆä¾› econml å¤ç”¨ï¼‰
        self.best_reg_ = CatBoostRegressor(
            iterations=self.best_params_["iterations"],
            depth=self.best_params_["reg_depth"],
            learning_rate=self.best_params_["reg_lr"],
            l2_leaf_reg=self.best_params_["reg_l2"],
            subsample=self.best_params_["reg_subsample"],
            random_seed=self.random_seed,
            verbose=False,
            loss_function=reg_lf,
        )

        self.best_clf_ = CatBoostClassifier(
            iterations=self.best_params_["iterations"],
            depth=self.best_params_["clf_depth"],
            learning_rate=self.best_params_["clf_lr"],
            l2_leaf_reg=self.best_params_["clf_l2"],
            subsample=self.best_params_["clf_subsample"],
            auto_class_weights=self.best_params_.get("clf_class_wt", None),
            random_seed=self.random_seed,
            verbose=False,
            loss_function="Logloss",
        )

        # è¿”å›â€œæœªæ‹Ÿåˆâ€çš„æœ€ä½³æ¨¡å‹ï¼ˆç”¨äºä¼ ç»™ econml learnerï¼‰
        return self.best_reg_, self.best_clf_
    
